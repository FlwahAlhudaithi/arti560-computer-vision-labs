{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430e7f7b",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d77e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f73ba53",
   "metadata": {},
   "source": [
    "## 1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "## You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458731ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = y_train.squeeze()\n",
    "y_test  = y_test.squeeze()\n",
    "\n",
    "IMG_SIZE = 96\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fbabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_mobilenet(x, y, training=False):\n",
    "    x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "    x = tf.image.resize(x, (IMG_SIZE, IMG_SIZE))\n",
    "    x = x * 255.0\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "\n",
    "    if training:\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\\\n",
    "    .shuffle(20000)\\\n",
    "    .map(lambda a,b: preprocess_for_mobilenet(a,b, True),\n",
    "         num_parallel_calls=AUTOTUNE)\\\n",
    "    .batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\\\n",
    "    .map(lambda a,b: preprocess_for_mobilenet(a,b, False),\n",
    "         num_parallel_calls=AUTOTUNE)\\\n",
    "    .batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad96042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base = tf.keras.applications.MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "base.trainable = False  # freeze for stage 1\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "mobilenet = keras.Model(inputs, outputs, name=\"MobileNetV2_CIFAR10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c5601",
   "metadata": {},
   "source": [
    "## 2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df025593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV2_CIFAR10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MobileNetV2_CIFAR10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_96             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_96             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 5\n",
      "Total params: 2270794\n",
      "Trainable: 12810\n",
      "Frozen: 2257984\n"
     ]
    }
   ],
   "source": [
    "mobilenet.summary()\n",
    "\n",
    "trainable_params = sum([np.prod(v.shape) for v in mobilenet.trainable_weights])\n",
    "frozen_params = sum([np.prod(v.shape) for v in mobilenet.non_trainable_weights])\n",
    "\n",
    "print(\"Depth:\", len(mobilenet.layers))\n",
    "print(\"Total params:\", trainable_params + frozen_params)\n",
    "print(\"Trainable:\", trainable_params)\n",
    "print(\"Frozen:\", frozen_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d791fcf",
   "metadata": {},
   "source": [
    "## Back to task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d976acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 91ms/step - accuracy: 0.7631 - loss: 0.7063 - val_accuracy: 0.8445 - val_loss: 0.4475\n",
      "Epoch 2/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 90ms/step - accuracy: 0.8430 - loss: 0.4589 - val_accuracy: 0.8521 - val_loss: 0.4250\n",
      "Epoch 3/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 90ms/step - accuracy: 0.8558 - loss: 0.4216 - val_accuracy: 0.8585 - val_loss: 0.4101\n",
      "Epoch 4/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 90ms/step - accuracy: 0.8617 - loss: 0.4037 - val_accuracy: 0.8556 - val_loss: 0.4193\n",
      "Epoch 5/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 90ms/step - accuracy: 0.8631 - loss: 0.3994 - val_accuracy: 0.8596 - val_loss: 0.4011\n"
     ]
    }
   ],
   "source": [
    "mobilenet.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "mobilenet.fit(train_ds, epochs=5, validation_data=test_ds)\n",
    "\n",
    "time_stage1 = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ce9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 150ms/step - accuracy: 0.8689 - loss: 0.4090 - val_accuracy: 0.8604 - val_loss: 0.4312\n",
      "Epoch 2/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 150ms/step - accuracy: 0.9355 - loss: 0.1882 - val_accuracy: 0.8772 - val_loss: 0.4275\n",
      "Epoch 3/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 149ms/step - accuracy: 0.9583 - loss: 0.1174 - val_accuracy: 0.8916 - val_loss: 0.3823\n",
      "Epoch 4/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 150ms/step - accuracy: 0.9757 - loss: 0.0728 - val_accuracy: 0.9050 - val_loss: 0.3228\n",
      "Epoch 5/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 150ms/step - accuracy: 0.9832 - loss: 0.0491 - val_accuracy: 0.9058 - val_loss: 0.3425\n"
     ]
    }
   ],
   "source": [
    "base.trainable = True\n",
    "\n",
    "FINE_TUNE_AT = 100\n",
    "for layer in base.layers[:FINE_TUNE_AT]:\n",
    "    layer.trainable = False\n",
    "\n",
    "mobilenet.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "mobilenet.fit(train_ds, epochs=5, validation_data=test_ds)\n",
    "\n",
    "time_stage2 = time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2f81fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.9058 - loss: 0.3425\n",
      "MobileNetV2 Accuracy: 0.9057999849319458\n",
      "Training Time: 475.1380388736725\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = mobilenet.evaluate(test_ds)\n",
    "total_time = time_stage1 + time_stage2\n",
    "\n",
    "print(\"MobileNetV2 Accuracy:\", test_acc)\n",
    "print(\"Training Time:\", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56751418",
   "metadata": {},
   "source": [
    "## 3. Then compare its performance with ResNet and the custom CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61230f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison (CIFAR-10)\n",
      "------------------------------------------------------------\n",
      "ResNet50V2 (fine-tuned)   | Accuracy: 0.9162\n",
      "MobileNetV2               | Accuracy: 0.9058\n",
      "Custom CNN                | Accuracy: 0.7028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_test_acc = 0.7027999758720398\n",
    "\n",
    "resnet_test_acc = 0.9161999821662903\n",
    "\n",
    "mobilenet_test_acc = 0.9057999849319458\n",
    "\n",
    "\n",
    "results = [\n",
    "    (\"ResNet50V2 (fine-tuned)\", resnet_test_acc),\n",
    "    (\"MobileNetV2\", mobilenet_test_acc),\n",
    "    (\"Custom CNN\", cnn_test_acc)\n",
    "]\n",
    "\n",
    "print(\"Model Comparison (CIFAR-10)\")\n",
    "print(\"-\" * 60)\n",
    "for name, acc in results:\n",
    "    print(f\"{name:<25} | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955042f",
   "metadata": {},
   "source": [
    "# 1) Accuracy Comparison\n",
    "- ResNet50V2 (fine-tuned): 0.9162  → highest accuracy\n",
    "- MobileNetV2 (fine-tuned):  0.9058\n",
    "- Basic CNN: 0.7028\n",
    "- ResNet50V2 achieved the best performance on CIFAR-10, followed closely by MobileNetV2. Both transfer learning models significantly outperform the basic CNN trained from scratch.\n",
    "\n",
    "\n",
    "# 2) Training Speed\n",
    "- The basic CNN trained the fastest because it has fewer layers and parameters.\n",
    "- MobileNetV2 required moderate training time due to its lightweight architecture.\n",
    "- ResNet50V2 was the slowest because of its deeper structure and larger number of parameters.\n",
    "\n",
    "\n",
    "# 3) Architecture Explanation\n",
    "- ResNet50V2 uses residual (skip) connections that allow very deep networks to learn effectively.\n",
    "- MobileNetV2 uses depthwise-separable convolutions, making it computationally efficient.\n",
    "- The basic CNN has a simple architecture with limited depth, which results in faster training, but lower accuracy compared to transfer learning models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
